{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the template code of using GNN to solve DST problem instances from different types of graph with different scales."
      ],
      "metadata": {
        "id": "kUG2QlpVCWqT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgIIHDsK9eIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce7e32f-7e1f-47b4-95f1-fbb962813291"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6SDYBU-sJF"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import collections\n",
        "import heapq\n",
        "import copy\n",
        "import math\n",
        "import queue\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.nn import Linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pages(num_pages):\n",
        "    G = nx.erdos_renyi_graph(num_pages, 0.3, directed=True) # a random graph w.p. 0.3\n",
        "    #G = nx.barabasi_albert_graph(num_pages, 10)\n",
        "    #G= nx.scale_free_graph(num_pages, alpha=0.75, beta=0.2, gamma=0.05, create_using=X)\n",
        "    adjacency = collections.defaultdict(set) # dictionary to store links of certain pages\n",
        "    size = dict() # to store size of each page\n",
        "\n",
        "    for i in G.nodes:\n",
        "        for j in list(G.adj[i]):\n",
        "            adjacency[i].add(j)\n",
        "        size[i] = abs(random.gauss(10, 6)) # randomly assign weights to each node\n",
        "    \n",
        "    homepage = 0\n",
        "\n",
        "    return homepage, adjacency, size"
      ],
      "metadata": {
        "id": "X100mkN096VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pages(vertices, m): # random power law\n",
        "    adjacency = collections.defaultdict(set) # dictionary to store links of certain pages\n",
        "    outdegree = collections.defaultdict(int)\n",
        "    size = dict() # to store size of each page\n",
        "    a = 1.72\n",
        "\n",
        "    for i in range(vertices): # id 0 to (vertices - 1)\n",
        "        out_degree = math.ceil((np.random.pareto(a,) + 1)*m)\n",
        "        while(out_degree > vertices):\n",
        "            out_degree = math.ceil((np.random.pareto(a,) + 1)*m)  \n",
        "        outdegree[i] = out_degree\n",
        "\n",
        "        while(out_degree):\n",
        "            link = random.randint(0, vertices - 1)\n",
        "            if link != i and link not in adjacency[i]:\n",
        "                adjacency[i].add(link)\n",
        "                out_degree -= 1\n",
        "\n",
        "    # max_node = max(outdegree, key=outdegree.get) \n",
        "    # adjacency[0], adjacency[max_node] =  adjacency[max_node], adjacency[0]\n",
        "    # if 0 in adjacency[0]:\n",
        "    #   adjacency[0].remove(0)\n",
        "    #   adjacency[0].add(max_node)\n",
        "    # if max_node in adjacency[max_node]:\n",
        "    #   adjacency[max_node].remove(max_node)\n",
        "    #   adjacency[max_node].add(0)\n",
        "\n",
        "    for v in adjacency.keys():\n",
        "        size[v] = round(abs(random.gauss(10, 6)), 2) # randomly assign size amount to each page\n",
        "    \n",
        "    homepage = 0\n",
        "\n",
        "    return homepage, adjacency, size"
      ],
      "metadata": {
        "id": "F_Xz3b_KsNTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfoVvJVd_IG"
      },
      "source": [
        "def Dijkstra(source):\n",
        "    global size\n",
        "    global adjacency\n",
        "    \n",
        "    linkedpages = set() # to store visited ones\n",
        "    queue = [source]\n",
        "    while(queue):\n",
        "        page = queue.pop()\n",
        "        linkedpages.add(page)\n",
        "        for p in adjacency[page]:\n",
        "            if p not in linkedpages:\n",
        "                queue.append(p)\n",
        "    \n",
        "    dist = {}\n",
        "    for page_id in linkedpages:\n",
        "        dist[page_id] = [float('inf'), str()] # slower when adding path, maybe caused by list structure\n",
        "    dist[source] = [0, str(source)]\n",
        "    Q = [] # a priority queue\n",
        "    S = set() # to store already visited page\n",
        "    heapq.heappush(Q, (dist[source][0], source, dist[source][1]))\n",
        "    N = len(linkedpages)\n",
        "    \n",
        "    while(len(S) < N):\n",
        "        dis, page_id, path = heapq.heappop(Q)\n",
        "        if page_id not in S:\n",
        "            S.add(page_id)\n",
        "            for next_page in adjacency[page_id]:\n",
        "                if next_page not in S:\n",
        "                    #dist[next_page][0] = min(dist[next_page][0], dis + size[next_page])\n",
        "                    if dis + size[next_page] < dist[next_page][0]:\n",
        "                        dist[next_page][0] = dis + size[next_page]\n",
        "                        dist[next_page][1] = path + '-' + str(next_page)\n",
        "                    heapq.heappush(Q, (dist[next_page][0], next_page, dist[next_page][1]))        \n",
        "    \n",
        "    return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czSZ13DmeB3y"
      },
      "source": [
        "def subsets(terminals):\n",
        "    allsubsets = []\n",
        "    print(len(terminals))\n",
        "    for i in range(int(math.pow(2, len(terminals)))):\n",
        "        subset = []\n",
        "        \n",
        "        for j in range(len(terminals)):\n",
        "            if (i&(1 << j) > 0):\n",
        "                subset.append(terminals[j])\n",
        "        \n",
        "        allsubsets.append(subset)\n",
        "    \n",
        "    return allsubsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPFsKYm5eEi7"
      },
      "source": [
        "def Transition(S):\n",
        "    visited = set()\n",
        "    while(Q):\n",
        "        dis, node = heapq.heappop(Q)\n",
        "#         if node not in visited:\n",
        "#             visited.add(node)\n",
        "        for last_node in indegree[node]:\n",
        "            if dp[node][S] + size[node] < dp[last_node][S]:\n",
        "                tmp = set([node])\n",
        "                for j in path[node][S]:\n",
        "                     tmp.add(j)\n",
        "                path[last_node][S] = tmp.copy()\n",
        "                dp[last_node][S] = dp[node][S] +size[node]\n",
        "                heapq.heappush(Q, (dp[last_node][S], last_node))\n",
        "#             for next_node in adjacency[node]:\n",
        "#                 if dp[next_node][S] + size[next_node] < dp[node][S]:\n",
        "#                     dp[node][S] =  dp[next_node][S] + size[next_node]\n",
        "#                     heapq.heappush(Q, (dp[node][S], node))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module): # GAT\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, h, last_h):\n",
        "        super(Net, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=h))\n",
        "        self.batch_norms.append(torch.nn.BatchNorm1d(h*hidden_channels))\n",
        "        self.lins.append(Linear(in_channels, h*hidden_channels))        \n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(h*hidden_channels, hidden_channels, heads=h))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(h*hidden_channels))\n",
        "            self.lins.append(Linear(h*hidden_channels, h*hidden_channels))        \n",
        "        self.convs.append(GATConv(h*hidden_channels, out_channels, heads=last_h, concat=False))\n",
        "        self.lins.append(Linear(h*hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv, batch_norm, lin in zip(self.convs[:-1], self.batch_norms, self.lins[:-1]):\n",
        "            x = conv(x, edge_index) + lin(x)\n",
        "            x = batch_norm(x)\n",
        "            x = F.relu(x)\n",
        "            #x = F.dropout(x, p=0.2, training=self.training)\n",
        "        \n",
        "        x = self.convs[-1](x, edge_index) + self.lins[-1](x)\n",
        "        x = torch.sigmoid(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "o5ANwf9yoCIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(data.x, data.edge_index), data.y)\n",
        "        total_loss += loss.item() * data.num_graphs        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)"
      ],
      "metadata": {
        "id": "gtnlSA81oCsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    #global check_1, check_2\n",
        "    model.eval()\n",
        "\n",
        "    ys, preds = [], []\n",
        "    count = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        # check_1.append(data.y)\n",
        "        # check_2.append(out)\n",
        "        predicted = (out > 0.5).float().cpu()\n",
        "        \n",
        "        target_y = data.y.numpy().tolist()\n",
        "        pred_y = predicted.numpy().tolist()\n",
        "\n",
        "        ys.append(data.y)\n",
        "        preds.append((out > 0.5).float().cpu())\n",
        "\n",
        "        flag = 1\n",
        "        for i in range(len(target_y)):\n",
        "          if pred_y[i] != target_y[i]:\n",
        "            flag = 0\n",
        "        \n",
        "        if flag:\n",
        "          count += 1\n",
        "    \n",
        "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()    \n",
        "    node_count = 0\n",
        "    for j in range(len(y)):\n",
        "      if y[j] == pred[j]:\n",
        "        node_count += 1\n",
        "\n",
        "    return count / len(test_loader.dataset), node_count / len(y)"
      ],
      "metadata": {
        "id": "kcgsuQxdoE9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZg3SLyzeH_p"
      },
      "source": [
        "scale = 100\n",
        "homepage, adjacency, size = generate_pages(scale) # the scale is 100\n",
        "#homepage, adjacency, size = generate_pages(scale, 2) # the scale is 100\n",
        "k = 3\n",
        "dist = Dijkstra(homepage)\n",
        "indegree = collections.defaultdict(set)\n",
        "for i in adjacency.keys():\n",
        "    for j in adjacency[i]:\n",
        "        indegree[j].add(i)\n",
        "\n",
        "dest_set = []\n",
        "result_set = []\n",
        "\n",
        "num_samples = 40000\n",
        "\n",
        "for _ in range(num_samples): # generate data \n",
        "    destinations = random.sample(dist.keys(), k) \n",
        "    while homepage in destinations or sorted(destinations) in dest_set:\n",
        "        destinations = random.sample(dist.keys(), k)\n",
        "    destinations = sorted(destinations)\n",
        "\n",
        "    # initalize\n",
        "    endstate = 1<<k\n",
        "    dp = [[float('inf') for i in range(endstate)] for _ in range(scale)]\n",
        "    count = 0\n",
        "    Q = []\n",
        "    for i in range(scale):\n",
        "        if i in destinations:\n",
        "            dp[i][1<<count] = 0\n",
        "            count += 1\n",
        "\n",
        "    path = [[set([i]) for _ in range(endstate)] for i in range(scale)]\n",
        "\n",
        "    # DP\n",
        "    for S in range(1, 1<<k):\n",
        "        for i in range(scale):\n",
        "            sub = (S-1)&S\n",
        "            while(sub):\n",
        "                if dp[i][sub]+dp[i][S^sub] < dp[i][S]:\n",
        "                    dp[i][S] = dp[i][sub]+dp[i][S^sub]\n",
        "                    tmp = set()\n",
        "                    for j in path[i][sub]:\n",
        "                        tmp.add(j)\n",
        "                    for j in path[i][S^sub]:\n",
        "                        tmp.add(j)\n",
        "                    path[i][S] = tmp.copy()\n",
        "                sub = (sub-1)&S\n",
        "\n",
        "            if dp[i][S] < float('inf'):\n",
        "                heapq.heappush(Q, (dp[i][S], i))\n",
        "\n",
        "        Transition(S)\n",
        "\n",
        "    result = dp[homepage][(1<<k)-1]\n",
        "    guidance = path[homepage][-1]\n",
        "    #print(guidance)\n",
        "    result_set.append(guidance)\n",
        "    dest_set.append(destinations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6iAvrM_eg_Y"
      },
      "source": [
        "node_feature = [] # node-weighted\n",
        "for i in range(len(dest_set)):\n",
        "  min_size = min(size.values())\n",
        "  max_size = max(size.values())\n",
        "  features = [[(x-min_size)/(max_size-min_size), 0] for x in size.values()]\n",
        "  features = [[x, 0] for x in size.values()]\n",
        "  features[0][1] = 1\n",
        "  for j in dest_set[i]:\n",
        "    features[j][1] = 1\n",
        "  node_feature.append(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40D2zKXKo4jU"
      },
      "source": [
        "edges = []\n",
        "for i in adjacency.keys():\n",
        "  for j in adjacency[i]:\n",
        "    edges.append([i, j])\n",
        "edge_index = torch.tensor(edges, dtype=torch.long) # Edge connectivity with shape [2, num_edges]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9paxPBgbEb-q"
      },
      "source": [
        "labels = [] # target labels\n",
        "for i in range(len(result_set)):\n",
        "  plain = [[0] for _ in range(scale)]\n",
        "  plain[0] = [1]\n",
        "  for j in result_set[i]:\n",
        "    plain[j] = [1]\n",
        "  \n",
        "  labels.append(plain)\n",
        "\n",
        "#y = torch.tensor(labels[0], dtype=torch.int64) # targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G6ZtH8iwh3e"
      },
      "source": [
        "train_dataset = []\n",
        "for i in range(int(num_samples*0.7)):\n",
        "  x = torch.tensor(node_feature[i], dtype=torch.float) # Node feature matrix with shape [num_nodes, num_node_features (2 here, size, flag)]\n",
        "  y = torch.tensor(labels[i], dtype=torch.float) # targets\n",
        "  data = Data(x=x, y=y, edge_index=edge_index.t().contiguous()) # one set\n",
        "  train_dataset.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G_jQy-yT5dM"
      },
      "source": [
        "test_dataset = []\n",
        "for i in range(int(num_samples*0.7), num_samples):\n",
        "  x = torch.tensor(node_feature[i], dtype=torch.float) # Node feature matrix with shape [num_nodes, num_node_features (2 here, size, flag)]\n",
        "  y = torch.tensor(labels[i], dtype=torch.float) # targets\n",
        "  data = Data(x=x, y=y, edge_index=edge_index.t().contiguous()) # one set\n",
        "  test_dataset.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifhPOIgUfvY"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Adjust batch size to change RAM usage\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba0DhECaU1ni"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(2, 64, 1, 6, 4, 6).to(device)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01) # Adam is faster but not stable "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    train_accuracy, train_node = test(train_loader) \n",
        "    test_accuracy, test_node = test(test_loader)\n",
        "    print('Epoch: {:02d}, Loss: {:.4f}, Node accuracy of train: {:.4f}, Node accuracy of test: {:.4f}, Path accuracy of Test: {:.4f}'.format(\n",
        "        epoch, loss, train_node, test_node, test_accuracy)) # the test_accuracy (of the entire graphs) is the metric of interest"
      ],
      "metadata": {
        "id": "oR0Bzc0Wz5t4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}